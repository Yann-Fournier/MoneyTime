{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "969b3219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b01142fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1w = pd.read_csv('../database/binance/1w/BTC-USDT-USDT.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1aa3026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1w['Avg_day'] = ((df_1w['close'] + df_1w['open'] + df_1w['high'] + df_1w['low']) / 4).round(2)\n",
    "df_1w['Avg_corps'] = ((df_1w['close'] + df_1w['open']) / 2).round(2)\n",
    "df_1w['Avg_meches'] = ((df_1w['high'] + df_1w['low']) / 2).round(2)\n",
    "df_1w['Pourc_Price_Evol_14d'] = (100 * df_1w['Avg_day'].shift(-14) / df_1w['Avg_day']).round(2) - 100\n",
    "df_1w['Label'] = np.where(df_1w['Pourc_Price_Evol_14d'] > 5, 1,np.where(df_1w['Pourc_Price_Evol_14d'] < -5, -1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2eb0f793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>Avg_day</th>\n",
       "      <th>Avg_corps</th>\n",
       "      <th>Avg_meches</th>\n",
       "      <th>Pourc_Price_Evol_14d</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1567382400000</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10412.65</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10391.63</td>\n",
       "      <td>3096.291</td>\n",
       "      <td>10201.07</td>\n",
       "      <td>10195.81</td>\n",
       "      <td>10206.33</td>\n",
       "      <td>-28.18</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1567987200000</td>\n",
       "      <td>10316.62</td>\n",
       "      <td>10475.54</td>\n",
       "      <td>9884.31</td>\n",
       "      <td>10302.22</td>\n",
       "      <td>110846.484</td>\n",
       "      <td>10244.67</td>\n",
       "      <td>10309.42</td>\n",
       "      <td>10179.92</td>\n",
       "      <td>-30.30</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1568592000000</td>\n",
       "      <td>10302.00</td>\n",
       "      <td>10353.81</td>\n",
       "      <td>9530.02</td>\n",
       "      <td>10023.04</td>\n",
       "      <td>160591.544</td>\n",
       "      <td>10052.22</td>\n",
       "      <td>10162.52</td>\n",
       "      <td>9941.92</td>\n",
       "      <td>-26.24</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1569196800000</td>\n",
       "      <td>8061.98</td>\n",
       "      <td>10046.91</td>\n",
       "      <td>7700.67</td>\n",
       "      <td>8041.96</td>\n",
       "      <td>279795.272</td>\n",
       "      <td>8462.88</td>\n",
       "      <td>8051.97</td>\n",
       "      <td>8873.79</td>\n",
       "      <td>-14.05</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1569801600000</td>\n",
       "      <td>8042.08</td>\n",
       "      <td>8499.00</td>\n",
       "      <td>7709.01</td>\n",
       "      <td>7852.79</td>\n",
       "      <td>257976.889</td>\n",
       "      <td>8025.72</td>\n",
       "      <td>7947.44</td>\n",
       "      <td>8104.00</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1570406400000</td>\n",
       "      <td>7851.01</td>\n",
       "      <td>8788.00</td>\n",
       "      <td>7764.42</td>\n",
       "      <td>8274.66</td>\n",
       "      <td>367955.166</td>\n",
       "      <td>8169.52</td>\n",
       "      <td>8062.84</td>\n",
       "      <td>8276.21</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1571011200000</td>\n",
       "      <td>8274.33</td>\n",
       "      <td>8408.62</td>\n",
       "      <td>7812.00</td>\n",
       "      <td>8218.23</td>\n",
       "      <td>508975.286</td>\n",
       "      <td>8178.30</td>\n",
       "      <td>8246.28</td>\n",
       "      <td>8110.31</td>\n",
       "      <td>5.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1571616000000</td>\n",
       "      <td>8218.23</td>\n",
       "      <td>10408.48</td>\n",
       "      <td>7172.76</td>\n",
       "      <td>9533.32</td>\n",
       "      <td>881463.175</td>\n",
       "      <td>8833.20</td>\n",
       "      <td>8875.78</td>\n",
       "      <td>8790.62</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1572220800000</td>\n",
       "      <td>9534.37</td>\n",
       "      <td>9930.13</td>\n",
       "      <td>8933.00</td>\n",
       "      <td>9197.88</td>\n",
       "      <td>697075.617</td>\n",
       "      <td>9398.84</td>\n",
       "      <td>9366.12</td>\n",
       "      <td>9431.56</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1572825600000</td>\n",
       "      <td>9197.86</td>\n",
       "      <td>9550.00</td>\n",
       "      <td>8669.85</td>\n",
       "      <td>9041.31</td>\n",
       "      <td>665125.812</td>\n",
       "      <td>9114.75</td>\n",
       "      <td>9119.58</td>\n",
       "      <td>9109.92</td>\n",
       "      <td>10.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1573430400000</td>\n",
       "      <td>9041.31</td>\n",
       "      <td>9072.91</td>\n",
       "      <td>8355.78</td>\n",
       "      <td>8504.04</td>\n",
       "      <td>632991.917</td>\n",
       "      <td>8743.51</td>\n",
       "      <td>8772.67</td>\n",
       "      <td>8714.35</td>\n",
       "      <td>12.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1574035200000</td>\n",
       "      <td>8504.13</td>\n",
       "      <td>8504.71</td>\n",
       "      <td>6762.73</td>\n",
       "      <td>6900.86</td>\n",
       "      <td>1196506.081</td>\n",
       "      <td>7668.11</td>\n",
       "      <td>7702.49</td>\n",
       "      <td>7633.72</td>\n",
       "      <td>20.27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1574640000000</td>\n",
       "      <td>6899.77</td>\n",
       "      <td>7850.00</td>\n",
       "      <td>6510.19</td>\n",
       "      <td>7387.38</td>\n",
       "      <td>1524870.441</td>\n",
       "      <td>7161.84</td>\n",
       "      <td>7143.58</td>\n",
       "      <td>7180.09</td>\n",
       "      <td>17.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1575244800000</td>\n",
       "      <td>7388.00</td>\n",
       "      <td>7800.00</td>\n",
       "      <td>7070.00</td>\n",
       "      <td>7508.20</td>\n",
       "      <td>983668.839</td>\n",
       "      <td>7441.55</td>\n",
       "      <td>7448.10</td>\n",
       "      <td>7435.00</td>\n",
       "      <td>-15.34</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1575849600000</td>\n",
       "      <td>7507.87</td>\n",
       "      <td>7676.48</td>\n",
       "      <td>7004.77</td>\n",
       "      <td>7115.00</td>\n",
       "      <td>766967.380</td>\n",
       "      <td>7326.03</td>\n",
       "      <td>7311.44</td>\n",
       "      <td>7340.62</td>\n",
       "      <td>-23.25</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1576454400000</td>\n",
       "      <td>7115.45</td>\n",
       "      <td>7517.61</td>\n",
       "      <td>6427.00</td>\n",
       "      <td>7502.92</td>\n",
       "      <td>1031936.164</td>\n",
       "      <td>7140.74</td>\n",
       "      <td>7309.18</td>\n",
       "      <td>6972.30</td>\n",
       "      <td>-14.86</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1577059200000</td>\n",
       "      <td>7502.92</td>\n",
       "      <td>7691.00</td>\n",
       "      <td>7078.89</td>\n",
       "      <td>7383.67</td>\n",
       "      <td>901439.862</td>\n",
       "      <td>7414.12</td>\n",
       "      <td>7443.30</td>\n",
       "      <td>7384.94</td>\n",
       "      <td>-13.05</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1577664000000</td>\n",
       "      <td>7383.68</td>\n",
       "      <td>7495.00</td>\n",
       "      <td>6863.44</td>\n",
       "      <td>7354.36</td>\n",
       "      <td>774521.268</td>\n",
       "      <td>7274.12</td>\n",
       "      <td>7369.02</td>\n",
       "      <td>7179.22</td>\n",
       "      <td>-4.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1578268800000</td>\n",
       "      <td>7354.36</td>\n",
       "      <td>8468.42</td>\n",
       "      <td>7345.00</td>\n",
       "      <td>8186.70</td>\n",
       "      <td>1484603.363</td>\n",
       "      <td>7838.62</td>\n",
       "      <td>7770.53</td>\n",
       "      <td>7906.71</td>\n",
       "      <td>-11.42</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1578873600000</td>\n",
       "      <td>8186.22</td>\n",
       "      <td>9205.30</td>\n",
       "      <td>8053.78</td>\n",
       "      <td>8705.85</td>\n",
       "      <td>1316693.038</td>\n",
       "      <td>8537.79</td>\n",
       "      <td>8446.04</td>\n",
       "      <td>8629.54</td>\n",
       "      <td>-14.06</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      open      high       low     close       volume  \\\n",
       "0   1567382400000  10000.00  10412.65  10000.00  10391.63     3096.291   \n",
       "1   1567987200000  10316.62  10475.54   9884.31  10302.22   110846.484   \n",
       "2   1568592000000  10302.00  10353.81   9530.02  10023.04   160591.544   \n",
       "3   1569196800000   8061.98  10046.91   7700.67   8041.96   279795.272   \n",
       "4   1569801600000   8042.08   8499.00   7709.01   7852.79   257976.889   \n",
       "5   1570406400000   7851.01   8788.00   7764.42   8274.66   367955.166   \n",
       "6   1571011200000   8274.33   8408.62   7812.00   8218.23   508975.286   \n",
       "7   1571616000000   8218.23  10408.48   7172.76   9533.32   881463.175   \n",
       "8   1572220800000   9534.37   9930.13   8933.00   9197.88   697075.617   \n",
       "9   1572825600000   9197.86   9550.00   8669.85   9041.31   665125.812   \n",
       "10  1573430400000   9041.31   9072.91   8355.78   8504.04   632991.917   \n",
       "11  1574035200000   8504.13   8504.71   6762.73   6900.86  1196506.081   \n",
       "12  1574640000000   6899.77   7850.00   6510.19   7387.38  1524870.441   \n",
       "13  1575244800000   7388.00   7800.00   7070.00   7508.20   983668.839   \n",
       "14  1575849600000   7507.87   7676.48   7004.77   7115.00   766967.380   \n",
       "15  1576454400000   7115.45   7517.61   6427.00   7502.92  1031936.164   \n",
       "16  1577059200000   7502.92   7691.00   7078.89   7383.67   901439.862   \n",
       "17  1577664000000   7383.68   7495.00   6863.44   7354.36   774521.268   \n",
       "18  1578268800000   7354.36   8468.42   7345.00   8186.70  1484603.363   \n",
       "19  1578873600000   8186.22   9205.30   8053.78   8705.85  1316693.038   \n",
       "\n",
       "     Avg_day  Avg_corps  Avg_meches  Pourc_Price_Evol_14d  Label  \n",
       "0   10201.07   10195.81    10206.33                -28.18     -1  \n",
       "1   10244.67   10309.42    10179.92                -30.30     -1  \n",
       "2   10052.22   10162.52     9941.92                -26.24     -1  \n",
       "3    8462.88    8051.97     8873.79                -14.05     -1  \n",
       "4    8025.72    7947.44     8104.00                 -2.33      0  \n",
       "5    8169.52    8062.84     8276.21                  4.51      0  \n",
       "6    8178.30    8246.28     8110.31                  5.11      1  \n",
       "7    8833.20    8875.78     8790.62                  2.21      0  \n",
       "8    9398.84    9366.12     9431.56                  3.20      0  \n",
       "9    9114.75    9119.58     9109.92                 10.41      1  \n",
       "10   8743.51    8772.67     8714.35                 12.78      1  \n",
       "11   7668.11    7702.49     7633.72                 20.27      1  \n",
       "12   7161.84    7143.58     7180.09                 17.91      1  \n",
       "13   7441.55    7448.10     7435.00                -15.34     -1  \n",
       "14   7326.03    7311.44     7340.62                -23.25     -1  \n",
       "15   7140.74    7309.18     6972.30                -14.86     -1  \n",
       "16   7414.12    7443.30     7384.94                -13.05     -1  \n",
       "17   7274.12    7369.02     7179.22                 -4.22      0  \n",
       "18   7838.62    7770.53     7906.71                -11.42     -1  \n",
       "19   8537.79    8446.04     8629.54                -14.06     -1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1w.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd33a3d",
   "metadata": {},
   "source": [
    "## NN LTSTM + attention + Convolution (ChatGPT)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914d663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yfour\\Documents\\EnvsPython\\MoneyTime\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_layer                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">164</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionLayer</span>)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_layer                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m164\u001b[0m │\n",
       "│ (\u001b[38;5;33mAttentionLayer\u001b[0m)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,605</span> (178.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,605\u001b[0m (178.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,605</span> (178.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m45,605\u001b[0m (178.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, LSTM, Dense, Layer, Multiply, Permute, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Mécanisme d'attention simple\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)               # (batch, time_steps, 1)\n",
    "        a = K.softmax(e, axis=1)                            # (batch, time_steps, 1)\n",
    "        output = x * a                                      # (batch, time_steps, features)\n",
    "        return K.sum(output, axis=1)                        # (batch, features)\n",
    "\n",
    "# Paramètres du modèle\n",
    "time_steps = 100\n",
    "features = 64\n",
    "\n",
    "# Entrée\n",
    "inputs = Input(shape=(time_steps, features))\n",
    "\n",
    "# Convolution 1D\n",
    "conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(inputs)\n",
    "\n",
    "# LSTM\n",
    "lstm_out = LSTM(64, return_sequences=True)(conv)\n",
    "\n",
    "# Attention\n",
    "attention_out = AttentionLayer()(lstm_out)\n",
    "\n",
    "# Couche de sortie\n",
    "output = Dense(1, activation='sigmoid')(attention_out)\n",
    "\n",
    "# Modèle\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Afficher le résumé\n",
    "model.summary()\n",
    "\n",
    "# Temps : 24.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34b2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5113 - loss: 0.6969\n",
      "Epoch 2/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.4946 - loss: 0.6932\n",
      "Epoch 3/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5293 - loss: 0.6925\n",
      "Epoch 4/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5358 - loss: 0.6919\n",
      "Epoch 5/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5422 - loss: 0.6916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1aec67f0e10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple de données fictives\n",
    "import numpy as np\n",
    "X = np.random.rand(1000, time_steps, features)\n",
    "y = np.random.randint(0, 2, 1000)\n",
    "\n",
    "model.fit(X, y, epochs=5, batch_size=32)\n",
    "\n",
    "# Temps : 4.4s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MoneyTime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
